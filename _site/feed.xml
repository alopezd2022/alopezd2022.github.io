<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-10-24T11:43:09+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">PRACTICES</title><entry><title type="html">Rescue people</title><link href="http://localhost:4000/2025/10/22/p2-rescue-people.html" rel="alternate" type="text/html" title="Rescue people" /><published>2025-10-22T00:00:00+02:00</published><updated>2025-10-22T00:00:00+02:00</updated><id>http://localhost:4000/2025/10/22/p2-rescue-people</id><content type="html" xml:base="http://localhost:4000/2025/10/22/p2-rescue-people.html"><![CDATA[<h2 id="description"><b>Description</b></h2>

<p>This practice consists of the detection of the survivors in an area with a coverage algorithm and a face detection algorithm.</p>

<h2 id="strategy"><b>Strategy</b></h2>

<h4 id="1º-transform-from-gps-to-utm"><b>1º Transform from GPS to UTM</b></h4>

<p>The last coordinate of the survivor are 40º30’29’‘N 40º30’29’‘W, but the dron doesn’t know about this coordinate system, so the coordinate must be transformed to UTM and metros.</p>

<h4 id="2º-coverage-algorithm"><b>2º Coverage Algorithm</b></h4>

<p>The coverage algorithm consists of going through a defined area creating squares with increasingly smaller sides.</p>

<div style="text-align: center;">
    <img src="/assets/images/p2_servicios/coverage.jpeg" alt="sin erosión" style="width: 300px; display: inline-block; margin: 0 10px;" />
</div>

<p>In order to create a robust algorithm, I defined an huge area where the survivor could be. Then, the area is divided in four quadrants and each quadrant is traversed by the coverage algorithm mentioned above. The center of the area is the last known position of the survivor and the size of each quadrant is determinated by the operator depending on the flight time, the weather… If the size of the area is increased, there are more probabilities of rescuing more people.</p>

<div style="text-align: center;">
    <img src="/assets/images/p2_servicios/quadrants.jpeg" alt="sin erosión" style="width: 300px; display: inline-block; margin: 0 10px;" />
</div>

<h4 id="3º-face-detection-"><b>3º Face Detection </b></h4>

<p>Face detection is made by a module from Open CV called Haar Cascade <a href="https://docs.opencv.org/4.5.0/db/d28/tutorial_cascade_classifier.html" title="Haar Cascade: Face Detection">1</a></p>

<p>Some good practices include rotate each frame before trying to detect a face in it. There was also a issue where the algorithm sometimes detected a face in the water, so a color filtered was applied to remove blue tones, making the people detection more accurate.</p>

<p align="center">
  <img src="/assets/images/p2_servicios/salida.gif" width="500" />
</p>

<h4 id="4º-ficticial-battery"><b>4º Ficticial battery</b></h4>

<p>To make the simulation as realistic as possible, the dron’s battery is modeled. The dron’s battery lasts 15 minutes, and when it runs out, the dron returns to the base on the boat and waits for 5 seconds, to simulate charging.</p>

<h2 id="inconvenient-and-solutions"><b>Inconvenient and solutions</b></h2>

<p>Because of the quadrants has a significant size, the distance beetween each position is big as well. So I decided to divide the distance creating 
sub-points improving the position control and decreasing the speed between each point.</p>

<p>In order to improve the coverage algorithm and make sure that the dron visits each position commanded, I added a control time to ensure that the dron stays at a point during 0.5 seconds.</p>

<h2 id="demostration"><b>Demostration</b></h2>

<p>Demostration of people’s detection.</p>

<p>In the video, you will watch the dron’s detection in each quadrant. In the first quadrant, top left, the dron detects 3 people. In the second quadrant, bottom left, the dron detects 1 person and in the third quadrant, top right, the dron detects 2 people. I didn’t record the fourth quadrant because of the duration of the video and I knew that the dron had detected the 6 people.</p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p2_service/video_cortado.mp4" type="video/mp4" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<p>Demostration of charging the battery</p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p2_service/video_recarga_cortado.mp4" type="video/mp4" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>]]></content><author><name></name></author><category term="service-robotics" /><summary type="html"><![CDATA[Description]]></summary></entry><entry><title type="html">Localized Vacuum Cleaner</title><link href="http://localhost:4000/2025/10/12/p1-localizec-vacuum-cleaner.html" rel="alternate" type="text/html" title="Localized Vacuum Cleaner" /><published>2025-10-12T00:00:00+02:00</published><updated>2025-10-12T00:00:00+02:00</updated><id>http://localhost:4000/2025/10/12/p1-localizec-vacuum-cleaner</id><content type="html" xml:base="http://localhost:4000/2025/10/12/p1-localizec-vacuum-cleaner.html"><![CDATA[<p align="center">
  <img src="/assets/videos/p1_service/salida.gif" width="500" />
</p>

<h2 id="description"><b>Description</b></h2>
<p>In this practice, a <strong>BSA (Backtracking Spiral Algorithm)</strong> coverage algorithm is implemented and applied to the autonomous navigation of a vacuum robot. The goal is to achieve complete coverage of the free area while avoiding obstacles and optimizing the robot’s movements.</p>

<h2 id="strategy"><b>Strategy</b></h2>

<h4 id="1º-map-record"><b>1º Map record</b></h4>

<p>This first step consists of obtaining the relation between the pose 3d of the vacuum in the world and the pose 2D of the vacuum in the map.</p>

<p>This relation lets us convert from pose 3D to pose 2D and vice versa. In order to get this relation we need to calculate the transformation matrix.
Since the vacuum robot isn’t at the center of the map we can not create a simple relation and obtain a matrix from the rotation of the axes from world to map.</p>

<p>Therefore, we need to get some points (many as we can) in 3D and the corresponding points in 2D so we can calculate least square an get the matrix which relates both sets of points. The more points we get, the less error is generated.</p>

<h4 id="2º-creating-a-navigation-grid-map"><b>2º Creating a navigation grid map</b></h4>

<p>The second step consists of creating a grid map. Each grid cell has a slightly smaller than the robot’s size in pixels. Having the size of each grid cell we can determinate the size of the navigation grid map. Each grid have a value depending on the states of the grid, if the grid cell corresponds to a obstacle the value is 0, if the grid cell is free, the value is 1 and if the robot has been in that grid cell, the value is 2.</p>

<h4 id="3º-implementation-of-a-bsa-coverage-algorithm"><b>3º Implementation of a bsa coverage algorithm</b></h4>

<p><em>The coverage algorthim uses a grid-based model, and assures the complete coverage of non-occupied cells; partially occupied cells are not considere, is supported by two main concepts: covering of simple regions using a spiral-like path and linking of simple regions based on a backtracking mechanism</em> <a href="https://www.academia.edu/103916763/BSA_A_Complete_Coverage_Algorithm" title="BSA: A Complete Coverage Algorithm">1</a></p>

<p>As the article said, this algorthim uses a coverage based on spiral that goes around all the free cell. But what happens when the robot ends a spiral and all the surrounding cells are occupied?. This algorithm also implements critical points and return points. A critical point is a point without an exit, and a return point is the closest free point to the critical point. So when the robot is at a critical point, it must move to a return point and continue the algorithm from there.</p>

<p>In order to get the closest return point and the path to this point, we need to implement a backtracking mechanism such as BFS Algorithm, Breadth-First Search.
Breadth-First Search consists of expanding an unvisited node and its successors until the goal is reached. In this case the goal is all the return points, and the node start in the critical point and go around each cell and its successors until a cell which is a return point is found. This allows obtaining the closest return point and the full path from the critical point to the return point.</p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p1_service/bsa.mp4" type="video/mp4" />
      Tu navegador no soporta la reproducción de videos.
    </video>

</div>

<h4 id="4º-reactive-piloting-according-to-the-planned-route"><b>4º Reactive piloting according to the planned route</b></h4>

<p>The piloting of the robot involves simple movements in which it is controlled by its position, in each iteration there is a new goal, a new grid in the map to move to. The robot is controlled by a simple P control with proportional gains applied to the error both in the angle and in the distance to the goal.</p>

<h2 id="inconvenient-and-solutions"><b>Inconvenient and solutions</b></h2>

<p>To obtain the points to create the transformation matrix, I initially obtain ten points, which were sufficient to create the navegation grid and to run the BSA algorithm. However, when the robot had to move, the transformation matrix wasn’t accurate enough and the robot collided at certain points. The solution was to obtain more points to create an accurate matrix. The points were the robot collided were the one that I used to improve the matrix.</p>

<p>Also regarding to obstacle avoidance, a good approach is to dilate the obstacles in the map so the robot won’t collide with them in the real world. Therefore, I apply an erosion operation to the map, resulting in less white area and more black areas.</p>

<div style="text-align: center;">
    <img src="/assets/images/p1_servicios/mapa_sinerosion.png" alt="sin erosión" style="width: 300px; display: inline-block; margin: 0 10px;" />
    <img src="/assets/images/p1_servicios/mapa_erosionado.png" alt="erosionado" style="width: 300px; display: inline-block; margin: 0 10px;" />
</div>

<h2 id="demostration"><b>Demostration</b></h2>

<p>I propose two different solutions, the first one is the fastest solution (aprox 16 min).</p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p1_service/video_cortox4.mp4" type="video/mp4" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<p>The second one, as I said, is the lowest solution (aprox 30 min). The movement is pretty slow but we ensure that every space of the house is clean at all.</p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p1_service/output_4x_largo.mp4" type="video/mp4" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>]]></content><author><name></name></author><category term="service-robotics" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Albor rover</title><link href="http://localhost:4000/2025/05/05/albor-rover.html" rel="alternate" type="text/html" title="Albor rover" /><published>2025-05-05T00:00:00+02:00</published><updated>2025-05-05T00:00:00+02:00</updated><id>http://localhost:4000/2025/05/05/albor-rover</id><content type="html" xml:base="http://localhost:4000/2025/05/05/albor-rover.html"><![CDATA[<h2 id="albor"><b>ALBOR</b></h2>

<div style="text-align: center;">
  <img src="/assets/images/modelado/modelo_renderizado.png" alt="GIF divertido" width="400" />
</div>

<p> </p>

<p>Albor es un robot rover diseñado con un brazo de tipo scara y un compartimento con una capacidad de 3 cubos de 50 cm de arista. Este robot fue diseñado en blender. Y después fue implementado en ros2 para su uso en gazebo harmonic. Para poder utilizar el robot al final del post hay un enlace al github con los dos paquetes indispensables, descargalos y compilalos en tu workspace de ros jazzy.</p>

<p>Para la realizacion de este robot se ha hecho uso de:</p>
<ul>
  <li>Blender con phobos.</li>
  <li><a href="https://gazebosim.org/docs/latest/getstarted/">Gazebo harmonic</a></li>
  <li><a href="https://moveit.picknik.ai/main/index.html">Moveit</a></li>
  <li><a href="https://wiki.ros.org/teleop_twist_keyboard">Teleoperar el robot en gazebo</a></li>
</ul>

<p> </p>

<h2 id="arbol-de-transformada"><b>Arbol de transformada</b></h2>

<div style="text-align: center;">
  <img src="/assets/images/modelado/Captura desde 2025-05-08 10-30-38.png" alt="GIF divertido" width="1000" />
</div>

<p> </p>

<div style="text-align: center;">
  <img src="/assets/images/modelado/Captura_parteA.png" alt="GIF divertido" width="1000" />
</div>

<p> </p>

<h2 id="analisis-gráfico"><b>Analisis gráfico</b></h2>

<h3 id="gráfica-posición-de-las-ruedas-vs-tiempo">Gráfica Posición de las ruedas vs Tiempo</h3>

<div style="text-align: center;">
  <img src="/assets/images/modelado/tiempo_vs_posicionruedas.png" alt="GIF divertido" width="400" />
</div>
<p> </p>

<p>En el eje y se encuentra la posición en metros y en el eje x se encuentra el tiempo en segundos.</p>

<p>En esta grafica podemos observar que la posición de las ruedas no varia hasta que iniciamos el movimiento del robot para aproximarnos al cubo. Una vez que nos aproximamos al cubo, a la posición para realizar el pick and place, se mantiene en esa posición.</p>

<p> </p>

<h3 id="gráfica-aceleración-vs-tiempo">Gráfica Aceleración vs Tiempo</h3>

<div style="text-align: center;">
  <img src="/assets/images/modelado/tiempo_vs_aceleracion.png" alt="GIF divertido" width="400" />
</div>

<p> </p>

<p>En el eje y se encuantra la aceleración lineal en m/s² y en el eje x se encuentra el tiempo en segundos.</p>

<p>En esta grafica podemos ver dos diferencias principales, por un lado las aceleraciones en el eje ‘x’ y en el eje ‘y’ y por otro lado las aceleraciones en el eje z.</p>

<ul>
  <li>
    <p>La aceleración en el eje x observamos que es cero al principio de la ejecución, entre los segundos 30 y 60 aproximadamente se realiza un movimiento en este eje, esto se debe a las oscilaciones que podemos observar.Y Una vez acabado el moviento vuelve a cero.</p>
  </li>
  <li>
    <p>La aceleración en el eje y se comporta de la misma manera que la aceleración en el eje x.</p>
  </li>
  <li>
    <p>La aceleración en el eje z se encuentra constante en 10 m/s² esto se corresponde a la gravedad ya que el eje z apunta hacia arriba.</p>
  </li>
</ul>

<p> </p>

<h3 id="gráfica-gasto-vs-tiempo">Gráfica Gasto vs Tiempo</h3>

<div style="text-align: center;">
  <img src="/assets/images/modelado/gasto_vs_tiempo.png" alt="GIF divertido" width="600" />
</div>

<p> </p>

<p>En esta grafica observamos primeramente, desde casi 400 hasta 415, se realiza un primer moviento correspondiente a estirar el brazo scara para posicionar el brazo para coger el cubo. La siguiente oscilacion corresponde a otro moviento del bazo para posicionarlo en posicion para depositar el cubo en el compartimento del rover y volver a la posicion anterior. La siguiente oscilación es negativa y corresponde a la fuerxa negativa para bajar el brazo y agarrar el cubo y la oscilación posistiva corresponde a volver a subir el brazo con el cubo agarrado.</p>

<p> </p>

<h2 id="video-de-ejecución"><b>Video de ejecución</b></h2>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/modelado/video.mp4" type="video/mp4" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<p> 
 </p>

<h4 id="rosbag">Rosbag</h4>
<p><a href="https://urjc-my.sharepoint.com/:f:/g/personal/a_lopezd_2022_alumnos_urjc_es/EsfUU-JLf9REudeLFno0jcUBakbcHoI9GJ1tpp3oV6GYgg?e=yLijZk">enlace de descarga del rosbag</a></p>
<h4 id="github">Github</h4>
<p><a href="https://github.com/alopezd2022/albor_repo.git">enlace del github de la practica</a></p>

<p> </p>]]></content><author><name></name></author><category term="modelado-y-simulacion-de-robots" /><summary type="html"><![CDATA[ALBOR]]></summary></entry><entry><title type="html">P4 Follow Line</title><link href="http://localhost:4000/2024/12/03/p4-follow-line.html" rel="alternate" type="text/html" title="P4 Follow Line" /><published>2024-12-03T00:00:00+01:00</published><updated>2024-12-03T00:00:00+01:00</updated><id>http://localhost:4000/2024/12/03/p4-follow-line</id><content type="html" xml:base="http://localhost:4000/2024/12/03/p4-follow-line.html"><![CDATA[<h1 id="rust-eze"><b>RUST-EZE</b></h1>
<h2 id="descripción"><b>Descripción</b></h2>

<p>El objetivo de esta practica es diseñar un sigue lineas incorporando comunación IoT, controladores, Arduino Threads…</p>

<h2 id="hardware"><b>Hardware</b></h2>

<p>Para este practica se ha usado un Arduino UNO, una ESP32, un sensor de ultrasonidos HC-SR04, un sensor de infrarrojos y los actuadores en cuestión, los motores para las ruedas del coche.</p>

<h2 id="software"><b>Software</b></h2>

<p>Para esta practica hemos obstado por continuar con el sistema operativo de Arduino y no implementar FREE-RTOS. Esto se debe a que esta practica no precisa de un manejo estricto de tareas que deban cumplir deadlines. Con el único hilo y flujo de ejecución de Arduino se pueden conseguir los objetivos principales, seguir la linea leyendo en cada iteración el sensor de infrarrojos, enviar a la ESP la información necesaria mediante el puerto serie y verificar el sensor de ultrasonidos en cada iteración para la detección de obstáculos.</p>

<h3 id="-comunicación-"><b> Comunicación </b></h3>
<h4 id="serie-entre-arduino-y-esp"><b>Serie entre Arduino y ESP</b></h4>
<p>La comunicación por los puertos serie entre el Arduino y el ESP es una comunicación bidireccional, en la que la ESP inicia la comunicación indicando que se puede iniciar la carrera una vez que se ha conectado al WIFI y al broker MQTT. Cuando la ESP envía el mensaje de inicio, el Arduino inicia un paso de mensajes dependiendo de la acción que realiza el coche. Las acciones pueden ser :</p>
<ul>
  <li>START_LAP, inicio de la vuelta.</li>
  <li>END_LAP, fin de la vuelta, además envía el tiempo que ha tardado en dar la vuelta.</li>
  <li>OBSTACLE_DETECTED, detección de obstáculo, envía a que distancia ha detectado el obstáculo.</li>
  <li>LINE_LOST, perdida de linea.</li>
  <li>PING, cada 4 segundos se envía este mensaje junto con el tiempo transcurrido.</li>
  <li>LINE_FOUND, recuperación de linea.</li>
  <li>VISIBLE_LINE, se envía el porcentaje de linea que se ha visto durante la vuelta.</li>
  <li>INIT_LINE_SEARCH, inicia el algoritmo de búsqueda de linea.</li>
  <li>STOP_LINE_SEARCH, finaliza el algoritmo de búsqueda de linea.</li>
</ul>

<p>Para enviar todos estos mensajes, el arduino sigue el siguiente protocolo diseñado específicamente para la comunicación Arduino-ESP: <i>{/action:=$ACCION}</i>. Si ademas se quiere enviar algún dato más se añade tras la acción : <i>{/action:=$ACCION\/time:=$TIME}</i></p>

<h4 id="comunicación-iot-a-través-de-mqtt"><b>Comunicación IoT a través de MQTT</b></h4>

<p>Cuando la ESP32 recibe los distintos mensajes por el puerto serie esta recoge los mensajes, obtiene los datos necesarios y los envía en formato JSON al servidor MQTT en donde se procesan los datos y se lleva un control remoto de la vuelta.</p>

<h3 id="pid"><b>PID</b></h3>

<p>El control PID (Proporcional-Integral-Derivativo) es un algoritmo ampliamente utilizado en sistemas de control para ajustar una variable de salida en función de un valor deseado. En esta práctica, se utiliza un control proporcional-derivativo (PD), para guiar el movimiento del coche mientras sigue una línea negra detectada por sensores de infrarrojos.</p>

<p>El sistema de control PD se basa en los siguientes principios:</p>

<h4 id="componente-proporcional-p"><b>Componente Proporcional (P):<b></b></b></h4>

<p>Calcula una corrección basada en el error actual. En este caso, el error se define como la diferencia en intensidad de los valores de los sensores de infrarrojos a ambos lados de la línea. Este componente ajusta directamente las velocidades de los motores en función de la desviación de la línea, proporcionando una corrección inmediata.</p>

<h4 id="componente-derivativo-d"><b>Componente Derivativo (D):<b></b></b></h4>

<p>Ajusta la corrección en función del cambio del error en el tiempo. Este componente ayuda a suavizar los movimientos, evitando oscilaciones bruscas al anticipar el comportamiento del coche.</p>

<p>En el contexto de esta practica, el algoritmo realiza los siguientes pasos:</p>

<h4 id="1lectura-de-sensores"><b>1.Lectura de Sensores:<b></b></b></h4>

<p>Los sensores de infrarrojos detectan la intensidad de la línea en tres puntos: izquierda, centro y derecha. Según los valores recibidos, el sistema determina si el coche está alineado con la línea, desviado a la izquierda, o desviado a la derecha.</p>

<h4 id="2cálculo-del-error"><b>2.Cálculo del Error:<b></b></b></h4>

<p>Se calcula el error como la diferencia en las lecturas de los sensores izquierdo y derecho. Este error indica qué tan lejos está el coche de estar perfectamente alineado con la línea.</p>
<h4 id="3aplicación-del-control-pd"><b>3.Aplicación del Control PD:<b></b></b></h4>

<ul>
  <li>El error proporcional se multiplica por una constante de ganancia proporcional (Kp), lo que genera una corrección proporcional a la magnitud del error.</li>
  <li>El cambio del error (derivada del error) se multiplica por una constante de ganancia derivativa (Kd​), que suaviza las correcciones para evitar oscilaciones.</li>
</ul>

<h4 id="4ajuste-de-velocidades-de-los-motores"><b>4.Ajuste de Velocidades de los Motores:<b></b></b></h4>

<p>La corrección calculada se aplica para modificar las velocidades de los motores izquierdo y derecho, de manera que el coche pueda ajustar su trayectoria y mantenerse sobre la línea.</p>

<h4 id="5limitación-de-velocidades"><b>5.Limitación de Velocidades:<b></b></b></h4>

<p>Las velocidades calculadas se limitan para evitar valores fuera del rango permitido, asegurando un movimiento estable y seguro.</p>

<h2 id="video">Video</h2>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/follow_line/coche-clase.mp4" type="video/mp4" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<p> </p>

<p>Colaborador Jorge Barroso Saugar</p>]]></content><author><name></name></author><category term="embedded-and-real-time-systems" /><summary type="html"><![CDATA[RUST-EZE Descripción]]></summary></entry><entry><title type="html">P5 Monte Carlo Laser Localization</title><link href="http://localhost:4000/2024/12/02/p5-MCL.html" rel="alternate" type="text/html" title="P5 Monte Carlo Laser Localization" /><published>2024-12-02T00:00:00+01:00</published><updated>2024-12-02T00:00:00+01:00</updated><id>http://localhost:4000/2024/12/02/p5-MCL</id><content type="html" xml:base="http://localhost:4000/2024/12/02/p5-MCL.html"><![CDATA[<h2 id="description">Description</h2>

<p>The objective of this practice is to develop the algorithm of Monte Carlo to localizate the robot.</p>

<h2 id="monte-carlo-algorithm">Monte Carlo Algorithm</h2>

<h4 id="initialize">Initialize</h4>
<div style="display: flex; align-items: center;">
    <img src="/assets/videos/p5/inicializar.gif" alt="GIF divertido" width="400" />
    <div style="margin-left: 20px; text-align: right;">
        <p>
            The particles are initialized with a uniform random distribution with a noise:
            <br />
            <strong>INIT_XY_STD = 0.5</strong> <br />
            <strong>INIT_ANGLE_STD = 0.1</strong>
        </p>
    </div>
</div>

<p> </p>

<h4 id="step-1-propagation-motion-update">Step 1 Propagation (motion update)</h4>
<div style="display: flex; align-items: center;">
    <img src="/assets/videos/p5/propagacion.gif" alt="GIF divertido" width="400" />
    <div style="margin-left: 20px; text-align: right;">
        <p>
            The pose of particles move according to the movement of the robot with a little noise:
            <br />
            <strong>PROPAGATION_XY_NOISE_STD = 0.05</strong> <br />
            <strong>PROPAGATION_ANGLE_NOISE_STD = 0.01</strong>
        </p>
    </div>
</div>

<p> </p>

<h4 id="step-2-sensor-update-weight-estimation">Step 2 Sensor update (weight estimation)</h4>

<p>Following the Probabilistic Sensor Observation Model, the particles are assigned a weight depending on the difference between the laser sensor reading from the robot and a theoretical laser developed for each particle and each iteration.</p>

<p>The calculation of the probabilities is given by the following formula :</p>

<p><img src="/assets/images/p5/formula.png" alt="GIF divertido" width="400" /></p>

<p>Where the error represents the discrepancy between the actual sensor readings and the theoretical predictions for each particle, calculated based on their position and orientation in the map</p>

<p>The weights are normalized to ensure they form a probability distribution, making it easier to compare and interpret the likelihood of each particle accurately.</p>

<h4 id="step-3-resampling">Step 3 Resampling</h4>

<p>Select and give more importance to the most relevant particles, those which have more probability. Eliminating the irrelevant particles and duplicating the relevant ones</p>

<p> </p>

<h3 id="solution">Solution</h3>
<p>The most significant problem was the convergence of the particles, not because the algorithm wasn’t accurate, but because the computation time was very high, which did not allow for fast processing. Therefore, we can observe an initial solution without parallel processing and a final solution implementing parallel processing.</p>

<h4 id="result-without-multiprocessing">Result without multiprocessing</h4>

<div style="display: flex; align-items: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p5/sin_multi_video.mp4" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>
    <div style="margin-left: 20px; text-align: right;">
        <p>
            As we can see in the video, the computation time is quite high. This prevents the algorithm from fully converging, as it requires fast processing. Using fewer        particles and fewer virtual ray traces reduces the computation time, but it is still not enough.
            <br />
            <strong>N_PARTICLES = 500</strong> <br />
            <strong>LASER_NUM_BEAMS = 3</strong>
        </p>
    </div>
</div>

<p> </p>

<h4 id="result-with-multiprocessing">Result with multiprocessing</h4>

<div style="display: flex; align-items: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p5/multiprocesing_video.mp4" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>
    <div style="margin-left: 20px; text-align: right;">
        <p>
            By the other side, using multiprocessing to execute the virtuals ray traces parallel show that the computation time reduce significantly. So we can increase the number of virtual ray traces.
            <br />
            <strong>N_PARTICLES = 500</strong> <br />
            <strong>LASER_NUM_BEAMS = 5</strong>
        </p>
    </div>
</div>]]></content><author><name></name></author><category term="mobile-robotics" /><summary type="html"><![CDATA[Description]]></summary></entry><entry><title type="html">P3 Vending machine</title><link href="http://localhost:4000/2024/11/07/p3-maquina-expendedora.html" rel="alternate" type="text/html" title="P3 Vending machine" /><published>2024-11-07T00:00:00+01:00</published><updated>2024-11-07T00:00:00+01:00</updated><id>http://localhost:4000/2024/11/07/p3-maquina-expendedora</id><content type="html" xml:base="http://localhost:4000/2024/11/07/p3-maquina-expendedora.html"><![CDATA[<h2 id="description">Description</h2>
<p>La practica consiste en diseñar un controlador para una maquina expendedora que este basado en Arduino Uno. Para diseñar este controlador haremos uso de Arduino Threads, interrupciones, watchdog…</p>

<h2 id="hardware">Hardware</h2>

<p align="center">
  <img src="/assets/images/p3-empotrados/arduino.jpeg" alt="Arduino UNO" width="200" />
  <img src="/assets/images/p3-empotrados/lcd.webp" alt="LCD" width="200" />
  <img src="/assets/images/p3-empotrados/joystick.jpg" alt="Joystick" width="200" />
</p>
<p align="center">
  <b>Arduino UNO</b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <b>LCD</b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <b>Joystick</b>
</p>

<p align="center">
  <img src="/assets/images/p3-empotrados/dht11.jpeg" alt="Sensor DHT-11" width="200" />
  <img src="/assets/images/p3-empotrados/Hc-sr04.jpg" alt="Sensor HC-SR04" width="200" />
  <img src="/assets/images/p3-empotrados/boton.jpeg" alt="Botón" width="200" />
</p>
<p align="center">
  <b>Sensor DHT-11</b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <b>HC-SR04</b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <b>Botón</b>
</p>

<p align="center">
  <img src="/assets/images/p3-empotrados/leds.jpg" alt="LED Rojo" width="200" />
</p>
<p align="center">
  <b>LEDs</b> 
</p>

<p> </p>

<p>A partir de todos estos componentes obtenemos el siguiente esquema electrónico.</p>

<p align="center">
  <img src="/assets/images/p3-empotrados/Maquina_expendedora_bb.jpg" width="500" />
</p>

<h2 id="software">Software</h2>

<h3 id="clases">Clases</h3>
<h4 id="boot">Boot</h4>

<p>La clase boot es la clase encargada del arranque de la maquina expendedora. La clase Boot es una clase derivada de Thread que controla un pin led y una pantalla LCD, por lo tanto se ejecuta como “hilo independiente” que, una vez que acaba la secuencia de arranque, no se vuelve a ejecutar.</p>

<p>Al ejecutar se llama al metodo run la cual va alternando el estado de un pin y además muestra por la pantalla del lcd la cadena “CARGANDO…”.</p>

<p>Cuando el contador ha llegado a 3, se dejada de ejecutar el hilo y se remueve del controlador, ya que no volveremos a ejecutar dicha clase.</p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p3-empotrados/boot.mp4" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<p> </p>

<h4 id="sensors">Sensors</h4>

<p>La clase sensor es la clase encargada de la lectura del sensor de humedad y temperatura y humedad. La clase es una clase derivada de Thread, por lo que se ejecuta como “hilo” cada 5 segundos y nunca es removido debido a que siempre queremos tener monitorizado la temperatura y humedad de nuestra maquina para prevenir de posibles desastres.</p>

<p>En ella encontramos la inicialización del sensor, dos métodos que obtienen los valores de temperatura y humedad respectivamente y dos métodos que muestran los valores por el lcd.</p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p3-empotrados/sensors.mp4" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<p> </p>

<h4 id="menu">Menu</h4>

<p>La clase Menu es una clase que hereda de Thread y administra un menú de bebidas para una pantalla LCD controlada por un objeto LiquidCrystal. Contiene un arreglo de objetos Item (con nombre y precio), índices para la navegación del menú, y métodos para mostrar el menú en pantalla, cambiar precios y apagar la pantalla. El constructor inicializa el menú con 5 elementos predeterminados y configura el índice inicial en 0.</p>

<p>Los métodos permiten visualizar el elemento actual del menú, actualizar su precio y limpiar la pantalla LCD.</p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p3-empotrados/menu.mp4" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<p> </p>

<h4 id="admin">Admin</h4>

<p>La clase Admin hereda de Thread y gestiona un menú administrativo para un dispositivo con pantalla LCD controlada por un objeto LiquidCrystal. Contiene un arreglo de cadenas con 4 opciones de menú y un índice para navegar entre ellas.</p>

<p>Incluye un método para mostrar la opción actual del menú en la pantalla y otro para apagar (limpiar) la pantalla. Proporciona una interfaz simple para administrar funcionalidades como sensores y configuración.</p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p3-empotrados/admin.mp4" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<p> </p>

<h3 id="main">Main</h3>

<p>El main esta compuesto principalmente de las funciones :</p>

<ul>
  <li>
    <p><strong>setup</strong> en donde se inicializa el lcd, los pines para los leds y para las interrupciones, los intervalos de los threads en el controlador y el watchdog.</p>
  </li>
  <li>
    <p><strong>loop</strong> en donde se encuentra una maquina de estados finita.</p>
  </li>
</ul>

<p> </p>

<p align="center">
  <img src="/assets/images/p3-empotrados/state-machine.jpg" width="500" />
</p>

<p> </p>

<p>Como principales estados tenemos:</p>

<ul>
  <li>
    <p><strong>BOOT</strong>, estado que obtiene el contador de la clase BOOT y si ha llegado a 3 termina y transita al estado WAITING_CLIENT</p>
  </li>
  <li>
    <p><strong>WAITING_CLIENT</strong>, estado en el que el sensor de ultrasonidos recoge los valores de la distancia, en cuando hay un cliente a menos de un metro transita al estado IS_CLIENT</p>
  </li>
  <li>
    <p><strong>IS_CLIENT</strong>, estado que muestra durante 5 segundos la temperatura y humedad y transita al estado SHOW_MENU.</p>
  </li>
  <li>
    <p><strong>SHOW_MENU</strong>, estado que muestra el menú usando el método “showMenu” de la clase Menu, solo transita unicamente con la interrupción del botón del joystick.</p>
  </li>
  <li>
    <p><strong>PREPARE_PRODUCT</strong>, estado que simula la preparación del producto, en el se enciende progresivamente un led y muestra un mensaje por el lcd durnate un tiempo ramdon. Una vez finalizado muestra por el lcd que el producto esta listo y transita al estado WAITING_CLIENT.</p>
  </li>
</ul>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p3-empotrados/prepare-product.mp4" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<p> </p>

<p>En cualquier momento si queremos reiniciar y volver al estado WAITING_CLIENT se debe pulsar el botón en un rango de entre 2 y 3 segundos.</p>

<p>Con estos 5 estados tenemos el comportamiento a nivel de usuario de la maquina expendedora. Sin embargo, hay más comportamientos a nivel de administrados. Para transitar al estado de administrador se debe pulsar el botón durante 5 segundos.</p>

<ul>
  <li>
    <p><strong>ADMIN</strong>, estado que muestra el menú del administrados con las diferentes funcionalidades. Cada una se muestra como un sub-estado:</p>

    <ul>
      <li>
        <p><strong>SEE_TEMPERATURE</strong>, estado que muestra la temperatura y la humedad.</p>
      </li>
      <li>
        <p><strong>SEE_DISTANCE</strong>, estado que muestra los valores del sensor de ultrasonidos</p>
      </li>
      <li>
        <p><strong>COUNTER</strong>, estado que muestra el tiempo desde que se inicio.</p>
      </li>
      <li>
        <p><strong>CHANGE_PRICE</strong>, estado que cambia el precio de un producto, el cual tiene el sub-estado <strong>ASIGN_PRICE</strong> al cual solo transita unicamente con la interrupción del boton del joystick.</p>
      </li>
    </ul>
  </li>
</ul>

<p>Para transitar por los diferentes sub-estados al estado ADMIN unicamente se puede hacer mediante un movimiento del joystick en el eje X a la izquierda, una acción para “volver al menú”</p>

<p> </p>

<h4 id="watchdog">Watchdog</h4>

<p>Implementamos el watchdog como mecamismo de seguridad por si el sistema se queda bloqueado y no tiene el comportamiento deseado con un timer de 8 segundos.</p>

<p>Podemos observar su correcto funcionamiento ya que en el estado PREPARE_PRODUCT simulamos la preparación del producto con un delay random entre 4 y 8 segundos. Si no se resetea el watchdog, este va a reiniciar el sistema cuando el random de valor 8. En el siguiente video se puede observar lo comentado anteriormente.</p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p3-empotrados/watchdog.mp4" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>
<p> </p>

<h3 id="final-video">Final video</h3>

<p>Video final con el comportamiento completo de la Maquina expendedora.</p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p3-empotrados/complete-video.mp4" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<p> </p>

<h2 id="conclusion">Conclusion</h2>

<p>En conclusión, he obtado por esta implementación ya que el uso de <strong>clases</strong> nos facilita la posible implementación en otros proyectos, además de brindar mayor escalabilidad y modularidad a nuestro sistema, lo que permite un mantenimiento y actualización más eficientes. Por otro lado, el uso de <strong>threads</strong> nos permite ejecutar tareas en distintos periodos de tiempo sin necesidad de utilizar <strong>delays</strong>, mejorando significativamente la funcionalidad y la respuesta de nuestro <strong>sketch</strong>. Asi mismo, la implementación de <strong>interrupciones</strong> resulta fundamental para manejar eventos críticos en tiempo real, ya que estas permiten responder de manera inmediata a señales como los botones sin la necesidad de sondear constantemente, optimizando así el rendimiento del sistema y garantizando una mayor eficiencia.</p>]]></content><author><name></name></author><category term="embedded-and-real-time-systems" /><summary type="html"><![CDATA[Description La practica consiste en diseñar un controlador para una maquina expendedora que este basado en Arduino Uno. Para diseñar este controlador haremos uso de Arduino Threads, interrupciones, watchdog…]]></summary></entry><entry><title type="html">P4 Global Navigation</title><link href="http://localhost:4000/2024/10/28/p4-global-navigation.html" rel="alternate" type="text/html" title="P4 Global Navigation" /><published>2024-10-28T00:00:00+01:00</published><updated>2024-10-28T00:00:00+01:00</updated><id>http://localhost:4000/2024/10/28/p4-global-navigation</id><content type="html" xml:base="http://localhost:4000/2024/10/28/p4-global-navigation.html"><![CDATA[<h2 id="description">Description</h2>

<p>The objective of this practice is to develop a global navigation algorithm. The user marks a point on the map, and the car must reach it by the shortest path.</p>

<h2 id="hardware">Hardware</h2>

<p>We only have a car without any sensors. The only information we have is its current position.</p>

<h2 id="software">Software</h2>

<h3 id="wave-front-algorthim">Wave Front Algorthim</h3>
<p>For the car’s navigation, we developed a search algorithm based on the Wave Front Algorithm. This approach generates a cost map by expanding from the final target, assigned a cost of 0, to the car, which receives a cost of <em>n</em>.</p>

<p>To create the cost map, successors of each point are assigned a value equal to <em>cost + 1</em>, while diagonal successors are assigned a value of <em>cost + 1.41</em> (representing the Euclidean distance).</p>

<div style="text-align: center;">
    <img src="/assets/images/p4/Captura desde 2024-11-18 19-01-14.png" alt="car" style="width: 300px" />
</div>

<p>Additionally, obstacles and positions near them are assigned an extra cost to ensure that these points are avoided during navigation.</p>

<p>Once the search and the filling of the cost grid are completed, we display the map based on the cost grid. However, since the costs exceed 255 (the maximum white color), we need to normalize it so that it is displayed in a range from 0 to 255 based on the highest cost.</p>

<div style="text-align: center;">
    <img src="/assets/images/p4/Captura desde 2024-11-26 11-44-16.png" alt="car" style="width: 300px" />
</div>

<h3 id="gradient-algorthim">Gradient algorthim</h3>

<p>Once the cost map is generated, we implement a gradient-based navigation algorithm. It works using potential fields: obstacles act as “potential walls” that the robot avoids, while the target serves as a “potential well” that attracts it. By combining these walls and wells, a downward slope path is created, and the robot simply follows it to reach its destination.</p>

<p> </p>

<h3 id="videos">Videos</h3>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p4/grabacion-de-pantalla-desde-2024-11-18-10-37-27_hUPDu7c6.webm" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>

&nbsp;

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p4/grabacion-de-pantalla-desde-2024-11-18-10-44-35_0AQSLjZU.webm" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>

</div></div>]]></content><author><name></name></author><category term="mobile-robotics" /><summary type="html"><![CDATA[Description]]></summary></entry><entry><title type="html">P3 Obstacle Avoidance</title><link href="http://localhost:4000/2024/10/14/p3-obstacle_avoidance.html" rel="alternate" type="text/html" title="P3 Obstacle Avoidance" /><published>2024-10-14T00:00:00+02:00</published><updated>2024-10-14T00:00:00+02:00</updated><id>http://localhost:4000/2024/10/14/p3-obstacle_avoidance</id><content type="html" xml:base="http://localhost:4000/2024/10/14/p3-obstacle_avoidance.html"><![CDATA[<h2 id="description">Description</h2>

<p>The aim of this practice is to program a Formula 1 car which can avoid obstacle using Virtual Force Field system. In this case, we are going to develop a local navigating algorthims so that the car can move throw the circuit reaching the waypoints and avoiding the obstacles.</p>

<div style="text-align: center;">
    <img src="/assets/images/p3/coche.png" alt="car" style="width: 300px" />
</div>

<h2 id="hardware">Hardware</h2>

<p>The sensor is the laser LIDAR 180º and the actuators are as always the motor of the Formula 1 car.</p>

<h2 id="software">Software</h2>

<p>The strategy for this practice is develop a local navegation algorthim called VFF (Virtual Force Field). Is a navegation tecnice which allows our formula 1 car to move in a environment avoiding obstacles. Basically, the direction of the car is given by the attractive force generated by the position of the goal.</p>

<h3 id="vff">VFF</h3>
<p>The attractive vector is the vector drawn from the car’s position to the waypoint. This vector becomes very strong over long distances, so we will apply a force reducer.</p>

<p>The repulsive vector is the result of a weighted average of all the vectors drawn for all the angles of the laser. It is a weighted average because, in certain situations, such as very short distances, they do not have enough weight, and therefore the repulsive vector is not effective.</p>

<p>The resulting force should be the weighted average of both the attractive and repulsive vectors. Depending on the factors we assign to each, one vector will have more weight than the other. In this case, I found it more convenient to give greater weight to the repulsive vector so that it prioritizes avoiding obstacles.</p>

<h4 id="strategies-to-highlight">Strategies to highlight</h4>

<p>Reduce the size of the attractive vector, as at very high distances, it exerts too much force.</p>

<p>Increase the weight of small distances measured by the laser, so that when calculating the repulsive vector, these distances have more significance.</p>

<p>The linear velocity depends on the distance calculated as the magnitude of the resulting vector from the weighted average of both forces. This way, at short distances, the speed is reduced to allow for precise evasion.</p>

<p> </p>

<p>Here it is a video with the behavior of the formula 1 car.</p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p3/cochevueltacompleta.webm" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>

</div>]]></content><author><name></name></author><category term="mobile-robotics" /><summary type="html"><![CDATA[Description]]></summary></entry><entry><title type="html">P2 Follow Line</title><link href="http://localhost:4000/2024/09/30/p2-follow-line.html" rel="alternate" type="text/html" title="P2 Follow Line" /><published>2024-09-30T00:00:00+02:00</published><updated>2024-09-30T00:00:00+02:00</updated><id>http://localhost:4000/2024/09/30/p2-follow-line</id><content type="html" xml:base="http://localhost:4000/2024/09/30/p2-follow-line.html"><![CDATA[<h2 id="description">Description</h2>
<p>The objective of this practice is to create a line follower that completes the circuit in the shortest possible time. In this case, the Formula 1 car follows a red line, and a PD controller will be used to control the angular and linear speed of the system.</p>

<h2 id="hardware">Hardware</h2>

<p>In this practice, we have a Formula 1 car equipped with a camera to follow the line. Therefore, the sensor is the camera, and the actuators are the motors of the Formula 1 car.</p>

<div style="text-align: center;">
    <img src="/assets/images/p2/Captura desde 2024-10-11 10-03-10.png" alt="HSV" style="width: 300px" />
</div>

<h2 id="software">Software</h2>

<h3 id="the-color-filtering">The color filtering</h3>

<p>The color filtering is done using HSV, a system in which I can adjust the color range (in this case, red), brightness, and saturation of the color. With these three parameters, I can accurately control all shades of red, avoiding any external elements such as the incidence of unexpected light.</p>

<div style="text-align: center;">
    <img src="/assets/images/p2/gyuw4.png" alt="HSV" style="width: 500px" />
</div>
<p> </p>
<h3 id="strategy-1">Strategy 1</h3>
<p>My first strategy was to take a bounding box from the image and count how many red pixels there were on both the left and right sides. The difference between them would represent the error of our system. However, this algorithm was neither precise nor robust, as it did not account for situations like curves, where both the left and right sides could have the same number of pixels.</p>

<div style="text-align: center;">
    <img src="/assets/images/p2/Captura desde 2024-10-11 09-34-36.png" alt="HSV" style="width: 350px" />
</div>

<p>Since this solution was not valid, I decided to increase the size of the bounding box, but that didn’t work either. I even tried using the entire image, but it still didn’t work due to the same problem.</p>

<p> </p>
<h3 id="strategy-2">Strategy 2</h3>

<p>The second strategy consists of taking two lines, one horizontal and one vertical. For each line, the average of the red pixels at that moment is calculated. The point obtained from these two averages is the point the car should always follow, meaning it represents where the car should always be.</p>

<p>Therefore, the error in our system is the difference between the fixed center point of the image (the intersection of the horizontal and vertical lines) and the point obtained from the average of the red pixels, which indicates where the car should be. However, I chose to divide the error into two distinct errors, as the difference along the horizontal axis indicates how much the car needs to turn, while the difference along the vertical axis indicates how much the car needs to move forward.</p>

<p>With two errors—one for angular velocity and one for linear velocity—we obtain two different controllers, making our strategy more precise and robust.</p>

<p>The blue point seen in the image represents the average position of the red pixels in both the horizontal and vertical directions, while the drawn cross is the point where the car should be.</p>

<p>Additionally, when the car loses the line, it turns to the last saved angle.</p>

<div style="text-align: center;">
    <img src="/assets/images/p2/Captura desde 2024-10-10 17-11-46.png" alt="HSV" style="width: 350px" />
</div>

<p> </p>
<h3 id="control">Control</h3>

<p>We are going to implement a PD controller, which is a feedback control mechanism that adjusts the output based on the error obtained from the difference between the desired value and the current value. This controller uses two components to reduce the error and achieve a better system response: the proportional part and the derivative part.</p>

<p>Proportional</p>

<p> 
The proportional part adjusts the output in proportion to the error; that is, if the error is large, the controller’s output will also be large.</p>

<p>It is calculated as the error multiplied by the proportional constant Kp. The constant determines the intensity of the system’s output relative to the error.</p>

<p>Derivative
 </p>

<p>The derivative part measures the change in the error over time. It assesses how quickly the error is changing and adjusts the output based on that change.</p>

<p>It is calculated as the derivative of the error multiplied by a derivative constant.</p>

<h3 id="result">Result</h3>

<p>After explaining the filtering, tracking algorithm, and control, the result is a Formula 1 car that completes the circuit in approximately 100 seconds.</p>

<div style="text-align: center;">
    <video width="400" controls="">
      <source src="/assets/videos/p2/cochesimple.mp4" type="video/mp4" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<h2 id="conclusion">Conclusion</h2>

<p>Clearly, this algorithm is not the fastest, but in my opinion, it is quite robust and effective. I wasn’t looking for a fast car; I aimed for a car with control that has “smooth” movements, meaning it experiences minimal oscillation. As a final result, the car shows little oscillation and maintains control over both linear and angular velocity, which seems like a good outcome.</p>]]></content><author><name></name></author><category term="mobile-robotics" /><summary type="html"><![CDATA[Description The objective of this practice is to create a line follower that completes the circuit in the shortest possible time. In this case, the Formula 1 car follows a red line, and a PD controller will be used to control the angular and linear speed of the system.]]></summary></entry><entry><title type="html">P1 Vacuum Cleaner</title><link href="http://localhost:4000/2024/09/16/p1-vacuum-cleaner.html" rel="alternate" type="text/html" title="P1 Vacuum Cleaner" /><published>2024-09-16T00:00:00+02:00</published><updated>2024-09-16T00:00:00+02:00</updated><id>http://localhost:4000/2024/09/16/p1-vacuum-cleaner</id><content type="html" xml:base="http://localhost:4000/2024/09/16/p1-vacuum-cleaner.html"><![CDATA[<h2 id="description">Description</h2>
<p> 
The goal is to develop a software for a vacuum cleaner robot. The robot must explore the most surface of the house.
For that, we only have the laser sensor and the bumper; therefore the software for this application is not quite difficult.</p>

<p>Remember that the vacuum cleaner cannot make use of localization algorithms.</p>

<p> </p>
<h2 id="hardware">Hardware</h2>

<div style="text-align: center;">
    <img src="/assets/images/Captura desde 2024-09-27 15-52-32.png" alt="Aspiradora Robot" />
</div>
<p> </p>
<ul>
  <li>
    <p>As actuators, we have the wheels of the robot, which can move in a linear and angular way.</p>
  </li>
  <li>
    <p>As sensors, we have a laser LIDAR 180º and a bumper.</p>
  </li>
</ul>

<p> </p>
<h2 id="software">Software</h2>
<p> 
The system must be reactive, so that I have developed a state machine whose behavior is described below.
 </p>
<div style="text-align: center;">
    <img src="/assets/images/p1-vacuum-cleaner.drawio.png" alt="Aspiradora Robot" />
</div>
<p> 
As you can see, my solution to the problem is a reactive and random algorithm, in which the movements in the “Turn” and “Forward” states are random. However, when the robot collides and activates the bumper state, it first moves slightly backward and then has two options. One is “Turn max angle”, which, as the name suggests, uses the laser to find the direction with the greatest detected distance and turns towards it accordingly. The other option is “Turn half angle”, where the robot rotates on its axis and changes direction by turning approximately 180º.</p>

<h3 id="videos">Videos</h3>
<p> 
In this video, you can see the robot’s operation. The video is at 4x speed, so it will appear accelerated.
 </p>
<div style="text-align: center;">
    <video width="600" controls="">
      <source src="/assets/videos/Video-completo.webm" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>
<p> 
In this other video, you can see how the robot exits the room, avoiding getting trapped inside it.</p>
<div style="text-align: center;">
    <video width="400" controls="">
      <source src="/assets/videos/Video-habitaci%C3%B3n.webm" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>
<p> </p>

<h2 id="conclusion">Conclusion</h2>
<p>At first, when the practice was presented to us, I thought about creating complex software using geometric shapes that would cover the largest possible area. However, in reality, developing complex software to perform these tasks did not seem appropriate for this practice, as we had to develop a reactive algorithm. Therefore, I ultimately opted for a random algorithm that provides random movements and is constantly checking the sensors, which is where the reactivity comes from. This resulted in a simple algorithm for a sensory cleaning robot.</p>]]></content><author><name></name></author><category term="mobile-robotics" /><summary type="html"><![CDATA[Description   The goal is to develop a software for a vacuum cleaner robot. The robot must explore the most surface of the house. For that, we only have the laser sensor and the bumper; therefore the software for this application is not quite difficult.]]></summary></entry></feed>