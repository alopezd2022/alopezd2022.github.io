<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-11-24T12:36:38+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">PRACTICES</title><entry><title type="html">Logistic robot</title><link href="http://localhost:4000/2025/11/24/p4-amazon-warehouse.html" rel="alternate" type="text/html" title="Logistic robot" /><published>2025-11-24T00:00:00+01:00</published><updated>2025-11-24T00:00:00+01:00</updated><id>http://localhost:4000/2025/11/24/p4-amazon-warehouse</id><content type="html" xml:base="http://localhost:4000/2025/11/24/p4-amazon-warehouse.html"><![CDATA[<h2 id="description"><b>Description</b></h2>

<p>This practice consists of programming a <strong>logistic robot</strong> that holds the shelf and move to a determinate point at a warehouse.</p>

<p>For this purpose, <strong>OMPL</strong> is used to get the path for each point and a reactive control is used to move along the warehouse.</p>

<p>In this practice there are two different robots, <strong>holonomic</strong> and <strong>Ackermann</strong>. The first solution to this pratice is using the <strong>holonomic robot</strong> and applying <strong>differents geometries</strong> to validate the path. The second solution is using an <strong>Ackermann robot</strong> and applying <strong>differents geometries</strong> to validate the path.</p>

<h2 id="strategy"><b>Strategy</b></h2>

<p>The overall operation of the system is based on a <strong>state machine</strong> that changes dynamically:</p>

<div style="text-align: center;">
    <img src="/assets/images/p4_servicios/maquinaestados.png" alt="sin erosión" style="width: 300px; display: inline-block; margin: 0 10px;" />
</div>

<h3 id="1º-state-start-"><b>1º State: START </b></h3>

<p>In this phase, the global variable are initialize and the execution starts.</p>

<h3 id="2º-state-planning-"><b>2º State: PLANNING </b></h3>

<p>In this phase, get the actual pose of the robot in order to use it as the start point for the OMPL algorithm. 
If a plan is found, switch to the <strong>GOTO</strong> state.</p>

<h3 id="3º-state-goto-"><b>3º State: GOTO </b></h3>

<p>In this phase, the robot is controlled to reach a certain point using the function <strong>move_control</strong> which implements a P control. When a point is reached by the robot, the state switches to <strong>CHANGE_TARGET</strong> in which the target of the path is changed or if the path is complete switches to <strong>GET_SHELF</strong></p>

<h3 id="4º-state-get_shelf-"><b>4º State: GET_SHELF </b></h3>

<p>In this phase, the shelf is lifted and is deleted from the map. It is important because we get the new state of the warehouse because the shelf is no longer in that position and is going to be in other position.</p>

<h3 id="5º-state-return_shelf-"><b>5º State: RETURN_SHELF </b></h3>

<p>In this phase, the OMPL algorithm plan a path to place the shelf in a different position. And switches to <strong>GOTO</strong> to reach the new position.</p>

<h3 id="6º-state-leave_shelf-"><b>6º State: LEAVE_SHELF </b></h3>

<p>In this phase, the shelf is put down and the new position is added to the map. And a new shelf is selected to place to a diferrent position.</p>

<h3 id="ompl-implementation"><b>OMPL Implementation</b></h3>

<p>The space where the algorithm can plan is define, the start position and the goal position. The state validate function is also define in where we validate if a state is correct or not. In order to check the state, this function checks whether the robot can safely occupy a specific position on the map without colliding with anything. It first transforms the robot’s position and orientation into the map’s coordinate system. Then it estimates the <strong>geometry</strong> the robot (or the robot carrying a shelf) would occupy and verifies whether that space lies within a free area. If any part of that geometry falls outside the map or overlaps with an obstacle, the position is considered invalid. In essence, it is a collision-checking step that ensures the robot only evaluates movements that are <strong>physically feasible</strong>. And the planner used is <strong>RRTstar</strong>, which return the shortest and most optimal path.</p>

<ul>
  <li><b>HOLONOMIC </b></li>
</ul>

<p>In this case, the space is define by <strong>SE2StateSpace</strong>. And the size of the geometry to check in the validator is different because of robot’s size.</p>

<ul>
  <li><b>ACKERMANN </b></li>
</ul>

<p>In this case, the space is define by <strong>ReedsSheppStateSpace</strong> using a <strong>turning radius</strong>. And the size of the geometry to check in the validator is different because of robot’s size.</p>

<h2 id="demonstration"><b>Demonstration</b></h2>

<ul>
  <li><b>HOLONOMIC </b></li>
</ul>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p4_service/holonomic.mp4" type="video/mp4" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<ul>
  <li><b>ACKERMANN </b></li>
</ul>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p4_service/Ackermann1.mp4" type="video/mp4" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p4_service/Ackermann2.mp4" type="video/mp4" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>]]></content><author><name></name></author><category term="service-robotics" /><summary type="html"><![CDATA[Description]]></summary></entry><entry><title type="html">Autonomous Parking</title><link href="http://localhost:4000/2025/11/07/p3-autoparking.html" rel="alternate" type="text/html" title="Autonomous Parking" /><published>2025-11-07T00:00:00+01:00</published><updated>2025-11-07T00:00:00+01:00</updated><id>http://localhost:4000/2025/11/07/p3-autoparking</id><content type="html" xml:base="http://localhost:4000/2025/11/07/p3-autoparking.html"><![CDATA[<h2 id="description"><b>Description</b></h2>

<p>This practice consists of developing an <strong>autonomous system</strong> capable of <strong>detecting a parking space</strong> and <strong>performing the parking maneuver</strong> without human intervention.
For this purpose, data from <strong>laser sensors</strong> located at the <strong>front</strong>, <strong>rear</strong>, and <strong>sides</strong> of the vehicle are used, along with <strong>speed (V)</strong> and <strong>steering (W)</strong> control.</p>

<p>The algorithm implements different <strong>behavioral states</strong>, including: <strong>space search</strong>, <strong>alignment</strong>, <strong>parking space type identification</strong>, and <strong>parking maneuvers</strong> depending on the case.</p>

<h2 id="strategy"><b>Strategy</b></h2>

<p>The overall operation of the system is based on a <strong>state machine</strong> that changes dynamically according to the <strong>sensor readings</strong>.
Below are the main <strong>modules</strong> and <strong>phases</strong> of the practice.</p>

<h3 id="1º-state-searching-"><b>1º State: SEARCHING </b></h3>

<p>In this phase, the robot moves forward in a straight line while keeping itself aligned with the parked cars on its right.
To achieve this, the <code class="language-plaintext highlighter-rouge">get_align()</code> function compares the <strong>symmetric values</strong> from the <strong>right-side sensor</strong> to correct small deviations through the steering control <code class="language-plaintext highlighter-rouge">HAL.setW(w)</code>.</p>

<p>At the same time, the <code class="language-plaintext highlighter-rouge">search_parking()</code> function evaluates whether there is a sufficiently large space to park between two vehicles, using a geometry based on the <strong>Pythagorean theorem</strong>. Through this method, the <strong>angles</strong> and the <strong>longest side</strong> of the detected rectangle are calculated and compared with the <strong>laser readings</strong> to determine whether the space is free or not.
When a valid space is detected, the robot stops its forward motion and switches to the <strong>IDENTIFY</strong> state.</p>

<h3 id="2º-state-identify-"><b>2º State: IDENTIFY </b></h3>

<p>Once a potential parking space is detected, the system analyzes the distances at the <strong>extreme angles</strong> of the space (<code class="language-plaintext highlighter-rouge">min_angle</code> and <code class="language-plaintext highlighter-rouge">max_angle</code>) to determine the <strong>type of space</strong>:</p>

<ul>
  <li><strong>Type 0:</strong> there is a car in front and behind (intermediate space).</li>
  <li><strong>Type 1:</strong> no car in front (open space at the front).</li>
  <li><strong>Type 2:</strong> no car behind (open space at the rear).</li>
</ul>

<p>This identification is crucial to decide <strong>the direction of the maneuver</strong> and the <strong>movement mode</strong> (forward or reverse).</p>

<h3 id="3º-state-align-"><b>3º State: ALIGN </b></h3>

<p>In this state, the vehicle adjusts its position to become <strong>parallel to the parked cars</strong> before starting the maneuver.
The <code class="language-plaintext highlighter-rouge">align_with_car()</code> function once again uses data from the <strong>right-side sensor</strong> to maintain alignment and stops the movement once the <strong>geometry of the space</strong> matches the expected configuration.</p>

<h3 id="4º-state-manoeuvre-"><b>4º State: MANOEUVRE </b></h3>

<p>This state covers both <strong>parking with a car in front and another behind</strong> (type 0), and <strong>parking with only a car in front</strong> (type 2), since the movements are essentially the same.
The only difference lies in the <strong>reference system</strong> used when reversing:
when there are cars both in front and behind, the <strong>car behind</strong> is used as a reference while backing up; however, when there is only a car in front, that <strong>front car</strong> becomes the reference point.</p>

<p>The parking process is divided into <strong>substates</strong> that simulate a real parking maneuver:</p>

<ol>
  <li><strong>Reverse:</strong> the vehicle steers backward at a certain angle until it detects an increase in the lateral distance.</li>
  <li><strong>Forward:</strong> it continues reversing while correcting the steering angle until it reaches a minimum safe distance from the car behind or the car in front.</li>
  <li><strong>Straighten:</strong> it moves slightly forward while straightening the wheels to end up fully aligned within the parking space.</li>
</ol>

<p>At the end of this sequence, the vehicle is properly parked and stops.</p>

<h3 id="5º-state-manouvre_1-"><b>5º State: MANOUVRE_1 </b></h3>

<p>If the detected space is <strong>open at the front</strong> (type 1), the maneuver differs slightly:</p>

<ul>
  <li>The vehicle moves forward while turning to the right.</li>
  <li>When the lateral space increases enough, it straightens the wheels.</li>
  <li>Then, it reverses to center its position within the parking space.</li>
</ul>

<p>This procedure ensures <strong>efficient and safe parking</strong>, adapting to the different types of spaces encountered.</p>

<h2 id="inconvenients-and-solutions"><b>Inconvenients and Solutions</b></h2>

<ul>
  <li><strong>Infinite sensor readings:</strong> when the lasers did not detect any obstacles, they returned <code class="language-plaintext highlighter-rouge">inf</code>. This was solved using the <code class="language-plaintext highlighter-rouge">clean()</code> function, which replaces these values with the maximum measurable range.</li>
  <li><strong>Unstable alignment:</strong> small oscillations were observed when trying to maintain parallelism with the parked cars. This was stabilized by adjusting the proportional factor <code class="language-plaintext highlighter-rouge">k</code> of the error control.</li>
  <li><strong>Sudden substate changes:</strong> distance thresholds (<code class="language-plaintext highlighter-rouge">threshold_distance</code>) and short time delays were added to prevent incorrect transitions between phases.</li>
</ul>

<h2 id="demonstration"><b>Demonstration</b></h2>

<p>In the following videos, the complete sequence can be observed: <strong>space search</strong>, <strong>identification</strong>, and <strong>parking maneuver</strong>.</p>

<p><strong>Parking with a car in front and another behind</strong></p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p3_service/aparcar_delante_atras_recortado.mp4" type="video/mp4" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<p><strong>Parking with only a car behind</strong></p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p3_service/aparcar_para_atras_recortado.mp4" type="video/mp4" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<p><strong>Parking with only a car in front</strong></p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p3_service/aparcar_delante_recortado.mp4" type="video/mp4" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>]]></content><author><name></name></author><category term="service-robotics" /><summary type="html"><![CDATA[Description]]></summary></entry><entry><title type="html">Rescue people</title><link href="http://localhost:4000/2025/10/22/p2-rescue-people.html" rel="alternate" type="text/html" title="Rescue people" /><published>2025-10-22T00:00:00+02:00</published><updated>2025-10-22T00:00:00+02:00</updated><id>http://localhost:4000/2025/10/22/p2-rescue-people</id><content type="html" xml:base="http://localhost:4000/2025/10/22/p2-rescue-people.html"><![CDATA[<h2 id="description"><b>Description</b></h2>

<p>This practice consists of the detection of the survivors in an area with a coverage algorithm and a face detection algorithm.</p>

<h2 id="strategy"><b>Strategy</b></h2>

<h4 id="1º-transform-from-gps-to-utm"><b>1º Transform from GPS to UTM</b></h4>

<p>The last coordinate of the survivor are 40º30’29’‘N 40º30’29’‘W, but the dron doesn’t know about this coordinate system, so the coordinate must be transformed to UTM and metros.</p>

<h4 id="2º-coverage-algorithm"><b>2º Coverage Algorithm</b></h4>

<p>The coverage algorithm consists of going through a defined area creating squares with increasingly smaller sides.</p>

<div style="text-align: center;">
    <img src="/assets/images/p2_servicios/coverage.jpeg" alt="sin erosión" style="width: 300px; display: inline-block; margin: 0 10px;" />
</div>

<p>In order to create a robust algorithm, I defined an huge area where the survivor could be. Then, the area is divided in four quadrants and each quadrant is traversed by the coverage algorithm mentioned above. The center of the area is the last known position of the survivor and the size of each quadrant is determinated by the operator depending on the flight time, the weather… If the size of the area is increased, there are more probabilities of rescuing more people.</p>

<div style="text-align: center;">
    <img src="/assets/images/p2_servicios/quadrants.jpeg" alt="sin erosión" style="width: 300px; display: inline-block; margin: 0 10px;" />
</div>

<h4 id="3º-face-detection-"><b>3º Face Detection </b></h4>

<p>Face detection is made by a module from Open CV called Haar Cascade <a href="https://docs.opencv.org/4.5.0/db/d28/tutorial_cascade_classifier.html" title="Haar Cascade: Face Detection">1</a></p>

<p>Some good practices include rotate each frame before trying to detect a face in it. There was also a issue where the algorithm sometimes detected a face in the water, so a color filtered was applied to remove blue tones, making the people detection more accurate.</p>

<p>When our drone flies over the area, the downward-facing camera detects people on the ground. To locate each person precisely, we use the center of their face in the image as a reference.</p>

<p>First, the face detector gives us a bounding box around the face. If we rotated the image to improve detection, we transform those coordinates back to the original image.</p>

<p>Using the drone’s height and the camera’s field of view, we convert the face’s pixel position into real-world ground distances. Finally, adding the drone’s position gives us the exact coordinates of the person on the map, ready to mark their location or send alerts.</p>

<p align="center">
  <img src="/assets/images/p2_servicios/salida.gif" width="500" />
</p>

<h4 id="4-ficticial-battery"><b>4 Ficticial battery</b></h4>

<p>To make the simulation as realistic as possible, the dron’s battery is modeled. The dron’s battery lasts 15 minutes, and when it runs out, the dron returns to the base on the boat and waits for 5 seconds, to simulate charging.</p>

<h2 id="inconvenient-and-solutions"><b>Inconvenient and solutions</b></h2>

<p>Because of the quadrants has a significant size, the distance beetween each position is big as well. So I decided to divide the distance creating 
sub-points improving the position control and decreasing the speed between each point.</p>

<p>In order to improve the coverage algorithm and make sure that the dron visits each position commanded, I added a control time to ensure that the dron stays at a point during 0.5 seconds.</p>

<h2 id="demostration"><b>Demostration</b></h2>

<p>Demostration of people’s detection.</p>

<p>In the video, you will watch the dron’s detection in each quadrant. In the first quadrant, top left, the dron detects 3 people. In the second quadrant, bottom left, the dron detects 1 person and in the third quadrant, top right, the dron detects 2 people. I didn’t record the fourth quadrant because of the duration of the video and I knew that the dron had detected the 6 people.</p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p2_service/video_cortado.mp4" type="video/mp4" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<p>Demostration of charging the battery</p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p2_service/video_recarga_cortado.mp4" type="video/mp4" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>]]></content><author><name></name></author><category term="service-robotics" /><summary type="html"><![CDATA[Description]]></summary></entry><entry><title type="html">Localized Vacuum Cleaner</title><link href="http://localhost:4000/2025/10/12/p1-localizec-vacuum-cleaner.html" rel="alternate" type="text/html" title="Localized Vacuum Cleaner" /><published>2025-10-12T00:00:00+02:00</published><updated>2025-10-12T00:00:00+02:00</updated><id>http://localhost:4000/2025/10/12/p1-localizec-vacuum-cleaner</id><content type="html" xml:base="http://localhost:4000/2025/10/12/p1-localizec-vacuum-cleaner.html"><![CDATA[<p align="center">
  <img src="/assets/videos/p1_service/salida.gif" width="500" />
</p>

<h2 id="description"><b>Description</b></h2>
<p>In this practice, a <strong>BSA (Backtracking Spiral Algorithm)</strong> coverage algorithm is implemented and applied to the autonomous navigation of a vacuum robot. The goal is to achieve complete coverage of the free area while avoiding obstacles and optimizing the robot’s movements.</p>

<h2 id="strategy"><b>Strategy</b></h2>

<h4 id="1º-map-record"><b>1º Map record</b></h4>

<p>This first step consists of obtaining the relation between the pose 3d of the vacuum in the world and the pose 2D of the vacuum in the map.</p>

<p>This relation lets us convert from pose 3D to pose 2D and vice versa. In order to get this relation we need to calculate the transformation matrix.
Since the vacuum robot isn’t at the center of the map we can not create a simple relation and obtain a matrix from the rotation of the axes from world to map.</p>

<p>Therefore, we need to get some points (many as we can) in 3D and the corresponding points in 2D so we can calculate least square an get the matrix which relates both sets of points. The more points we get, the less error is generated.</p>

<h4 id="2º-creating-a-navigation-grid-map"><b>2º Creating a navigation grid map</b></h4>

<p>The second step consists of creating a grid map. Each grid cell has a slightly smaller than the robot’s size in pixels. Having the size of each grid cell we can determinate the size of the navigation grid map. Each grid have a value depending on the states of the grid, if the grid cell corresponds to a obstacle the value is 0, if the grid cell is free, the value is 1 and if the robot has been in that grid cell, the value is 2.</p>

<h4 id="3º-implementation-of-a-bsa-coverage-algorithm"><b>3º Implementation of a bsa coverage algorithm</b></h4>

<p><em>The coverage algorthim uses a grid-based model, and assures the complete coverage of non-occupied cells; partially occupied cells are not considere, is supported by two main concepts: covering of simple regions using a spiral-like path and linking of simple regions based on a backtracking mechanism</em> <a href="https://www.academia.edu/103916763/BSA_A_Complete_Coverage_Algorithm" title="BSA: A Complete Coverage Algorithm">1</a></p>

<p>As the article said, this algorthim uses a coverage based on spiral that goes around all the free cell. But what happens when the robot ends a spiral and all the surrounding cells are occupied?. This algorithm also implements critical points and return points. A critical point is a point without an exit, and a return point is the closest free point to the critical point. So when the robot is at a critical point, it must move to a return point and continue the algorithm from there.</p>

<p>In order to get the closest return point and the path to this point, we need to implement a backtracking mechanism such as BFS Algorithm, Breadth-First Search.
Breadth-First Search consists of expanding an unvisited node and its successors until the goal is reached. In this case the goal is all the return points, and the node start in the critical point and go around each cell and its successors until a cell which is a return point is found. This allows obtaining the closest return point and the full path from the critical point to the return point.</p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p1_service/bsa.mp4" type="video/mp4" />
      Tu navegador no soporta la reproducción de videos.
    </video>

</div>

<h4 id="4º-reactive-piloting-according-to-the-planned-route"><b>4º Reactive piloting according to the planned route</b></h4>

<p>The piloting of the robot involves simple movements in which it is controlled by its position, in each iteration there is a new goal, a new grid in the map to move to. The robot is controlled by a simple P control with proportional gains applied to the error both in the angle and in the distance to the goal.</p>

<h2 id="inconvenient-and-solutions"><b>Inconvenient and solutions</b></h2>

<p>To obtain the points to create the transformation matrix, I initially obtain ten points, which were sufficient to create the navegation grid and to run the BSA algorithm. However, when the robot had to move, the transformation matrix wasn’t accurate enough and the robot collided at certain points. The solution was to obtain more points to create an accurate matrix. The points were the robot collided were the one that I used to improve the matrix.</p>

<p>Also regarding to obstacle avoidance, a good approach is to dilate the obstacles in the map so the robot won’t collide with them in the real world. Therefore, I apply an erosion operation to the map, resulting in less white area and more black areas.</p>

<div style="text-align: center;">
    <img src="/assets/images/p1_servicios/mapa_sinerosion.png" alt="sin erosión" style="width: 300px; display: inline-block; margin: 0 10px;" />
    <img src="/assets/images/p1_servicios/mapa_erosionado.png" alt="erosionado" style="width: 300px; display: inline-block; margin: 0 10px;" />
</div>

<h2 id="demostration"><b>Demostration</b></h2>

<p>I propose two different solutions, the first one is the fastest solution (aprox 16 min).</p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p1_service/video_cortox4.mp4" type="video/mp4" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<p>The second one, as I said, is the lowest solution (aprox 30 min). The movement is pretty slow but we ensure that every space of the house is clean at all.</p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p1_service/output_4x_largo.mp4" type="video/mp4" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>]]></content><author><name></name></author><category term="service-robotics" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Albor rover</title><link href="http://localhost:4000/2025/05/05/albor-rover.html" rel="alternate" type="text/html" title="Albor rover" /><published>2025-05-05T00:00:00+02:00</published><updated>2025-05-05T00:00:00+02:00</updated><id>http://localhost:4000/2025/05/05/albor-rover</id><content type="html" xml:base="http://localhost:4000/2025/05/05/albor-rover.html"><![CDATA[<h2 id="albor"><b>ALBOR</b></h2>

<div style="text-align: center;">
  <img src="/assets/images/modelado/modelo_renderizado.png" alt="GIF divertido" width="400" />
</div>

<p> </p>

<p>Albor es un robot rover diseñado con un brazo de tipo scara y un compartimento con una capacidad de 3 cubos de 50 cm de arista. Este robot fue diseñado en blender. Y después fue implementado en ros2 para su uso en gazebo harmonic. Para poder utilizar el robot al final del post hay un enlace al github con los dos paquetes indispensables, descargalos y compilalos en tu workspace de ros jazzy.</p>

<p>Para la realizacion de este robot se ha hecho uso de:</p>
<ul>
  <li>Blender con phobos.</li>
  <li><a href="https://gazebosim.org/docs/latest/getstarted/">Gazebo harmonic</a></li>
  <li><a href="https://moveit.picknik.ai/main/index.html">Moveit</a></li>
  <li><a href="https://wiki.ros.org/teleop_twist_keyboard">Teleoperar el robot en gazebo</a></li>
</ul>

<p> </p>

<h2 id="arbol-de-transformada"><b>Arbol de transformada</b></h2>

<div style="text-align: center;">
  <img src="/assets/images/modelado/Captura desde 2025-05-08 10-30-38.png" alt="GIF divertido" width="1000" />
</div>

<p> </p>

<div style="text-align: center;">
  <img src="/assets/images/modelado/Captura_parteA.png" alt="GIF divertido" width="1000" />
</div>

<p> </p>

<h2 id="analisis-gráfico"><b>Analisis gráfico</b></h2>

<h3 id="gráfica-posición-de-las-ruedas-vs-tiempo">Gráfica Posición de las ruedas vs Tiempo</h3>

<div style="text-align: center;">
  <img src="/assets/images/modelado/tiempo_vs_posicionruedas.png" alt="GIF divertido" width="400" />
</div>
<p> </p>

<p>En el eje y se encuentra la posición en metros y en el eje x se encuentra el tiempo en segundos.</p>

<p>En esta grafica podemos observar que la posición de las ruedas no varia hasta que iniciamos el movimiento del robot para aproximarnos al cubo. Una vez que nos aproximamos al cubo, a la posición para realizar el pick and place, se mantiene en esa posición.</p>

<p> </p>

<h3 id="gráfica-aceleración-vs-tiempo">Gráfica Aceleración vs Tiempo</h3>

<div style="text-align: center;">
  <img src="/assets/images/modelado/tiempo_vs_aceleracion.png" alt="GIF divertido" width="400" />
</div>

<p> </p>

<p>En el eje y se encuantra la aceleración lineal en m/s² y en el eje x se encuentra el tiempo en segundos.</p>

<p>En esta grafica podemos ver dos diferencias principales, por un lado las aceleraciones en el eje ‘x’ y en el eje ‘y’ y por otro lado las aceleraciones en el eje z.</p>

<ul>
  <li>
    <p>La aceleración en el eje x observamos que es cero al principio de la ejecución, entre los segundos 30 y 60 aproximadamente se realiza un movimiento en este eje, esto se debe a las oscilaciones que podemos observar.Y Una vez acabado el moviento vuelve a cero.</p>
  </li>
  <li>
    <p>La aceleración en el eje y se comporta de la misma manera que la aceleración en el eje x.</p>
  </li>
  <li>
    <p>La aceleración en el eje z se encuentra constante en 10 m/s² esto se corresponde a la gravedad ya que el eje z apunta hacia arriba.</p>
  </li>
</ul>

<p> </p>

<h3 id="gráfica-gasto-vs-tiempo">Gráfica Gasto vs Tiempo</h3>

<div style="text-align: center;">
  <img src="/assets/images/modelado/gasto_vs_tiempo.png" alt="GIF divertido" width="600" />
</div>

<p> </p>

<p>En esta grafica observamos primeramente, desde casi 400 hasta 415, se realiza un primer moviento correspondiente a estirar el brazo scara para posicionar el brazo para coger el cubo. La siguiente oscilacion corresponde a otro moviento del bazo para posicionarlo en posicion para depositar el cubo en el compartimento del rover y volver a la posicion anterior. La siguiente oscilación es negativa y corresponde a la fuerxa negativa para bajar el brazo y agarrar el cubo y la oscilación posistiva corresponde a volver a subir el brazo con el cubo agarrado.</p>

<p> </p>

<h2 id="video-de-ejecución"><b>Video de ejecución</b></h2>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/modelado/video.mp4" type="video/mp4" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<p> 
 </p>

<h4 id="rosbag">Rosbag</h4>
<p><a href="https://urjc-my.sharepoint.com/:f:/g/personal/a_lopezd_2022_alumnos_urjc_es/EsfUU-JLf9REudeLFno0jcUBakbcHoI9GJ1tpp3oV6GYgg?e=yLijZk">enlace de descarga del rosbag</a></p>
<h4 id="github">Github</h4>
<p><a href="https://github.com/alopezd2022/albor_repo.git">enlace del github de la practica</a></p>

<p> </p>]]></content><author><name></name></author><category term="modelado-y-simulacion-de-robots" /><summary type="html"><![CDATA[ALBOR]]></summary></entry><entry><title type="html">P4 Follow Line</title><link href="http://localhost:4000/2024/12/03/p4-follow-line.html" rel="alternate" type="text/html" title="P4 Follow Line" /><published>2024-12-03T00:00:00+01:00</published><updated>2024-12-03T00:00:00+01:00</updated><id>http://localhost:4000/2024/12/03/p4-follow-line</id><content type="html" xml:base="http://localhost:4000/2024/12/03/p4-follow-line.html"><![CDATA[<h1 id="rust-eze"><b>RUST-EZE</b></h1>
<h2 id="descripción"><b>Descripción</b></h2>

<p>El objetivo de esta practica es diseñar un sigue lineas incorporando comunación IoT, controladores, Arduino Threads…</p>

<h2 id="hardware"><b>Hardware</b></h2>

<p>Para este practica se ha usado un Arduino UNO, una ESP32, un sensor de ultrasonidos HC-SR04, un sensor de infrarrojos y los actuadores en cuestión, los motores para las ruedas del coche.</p>

<h2 id="software"><b>Software</b></h2>

<p>Para esta practica hemos obstado por continuar con el sistema operativo de Arduino y no implementar FREE-RTOS. Esto se debe a que esta practica no precisa de un manejo estricto de tareas que deban cumplir deadlines. Con el único hilo y flujo de ejecución de Arduino se pueden conseguir los objetivos principales, seguir la linea leyendo en cada iteración el sensor de infrarrojos, enviar a la ESP la información necesaria mediante el puerto serie y verificar el sensor de ultrasonidos en cada iteración para la detección de obstáculos.</p>

<h3 id="-comunicación-"><b> Comunicación </b></h3>
<h4 id="serie-entre-arduino-y-esp"><b>Serie entre Arduino y ESP</b></h4>
<p>La comunicación por los puertos serie entre el Arduino y el ESP es una comunicación bidireccional, en la que la ESP inicia la comunicación indicando que se puede iniciar la carrera una vez que se ha conectado al WIFI y al broker MQTT. Cuando la ESP envía el mensaje de inicio, el Arduino inicia un paso de mensajes dependiendo de la acción que realiza el coche. Las acciones pueden ser :</p>
<ul>
  <li>START_LAP, inicio de la vuelta.</li>
  <li>END_LAP, fin de la vuelta, además envía el tiempo que ha tardado en dar la vuelta.</li>
  <li>OBSTACLE_DETECTED, detección de obstáculo, envía a que distancia ha detectado el obstáculo.</li>
  <li>LINE_LOST, perdida de linea.</li>
  <li>PING, cada 4 segundos se envía este mensaje junto con el tiempo transcurrido.</li>
  <li>LINE_FOUND, recuperación de linea.</li>
  <li>VISIBLE_LINE, se envía el porcentaje de linea que se ha visto durante la vuelta.</li>
  <li>INIT_LINE_SEARCH, inicia el algoritmo de búsqueda de linea.</li>
  <li>STOP_LINE_SEARCH, finaliza el algoritmo de búsqueda de linea.</li>
</ul>

<p>Para enviar todos estos mensajes, el arduino sigue el siguiente protocolo diseñado específicamente para la comunicación Arduino-ESP: <i>{/action:=$ACCION}</i>. Si ademas se quiere enviar algún dato más se añade tras la acción : <i>{/action:=$ACCION\/time:=$TIME}</i></p>

<h4 id="comunicación-iot-a-través-de-mqtt"><b>Comunicación IoT a través de MQTT</b></h4>

<p>Cuando la ESP32 recibe los distintos mensajes por el puerto serie esta recoge los mensajes, obtiene los datos necesarios y los envía en formato JSON al servidor MQTT en donde se procesan los datos y se lleva un control remoto de la vuelta.</p>

<h3 id="pid"><b>PID</b></h3>

<p>El control PID (Proporcional-Integral-Derivativo) es un algoritmo ampliamente utilizado en sistemas de control para ajustar una variable de salida en función de un valor deseado. En esta práctica, se utiliza un control proporcional-derivativo (PD), para guiar el movimiento del coche mientras sigue una línea negra detectada por sensores de infrarrojos.</p>

<p>El sistema de control PD se basa en los siguientes principios:</p>

<h4 id="componente-proporcional-p"><b>Componente Proporcional (P):<b></b></b></h4>

<p>Calcula una corrección basada en el error actual. En este caso, el error se define como la diferencia en intensidad de los valores de los sensores de infrarrojos a ambos lados de la línea. Este componente ajusta directamente las velocidades de los motores en función de la desviación de la línea, proporcionando una corrección inmediata.</p>

<h4 id="componente-derivativo-d"><b>Componente Derivativo (D):<b></b></b></h4>

<p>Ajusta la corrección en función del cambio del error en el tiempo. Este componente ayuda a suavizar los movimientos, evitando oscilaciones bruscas al anticipar el comportamiento del coche.</p>

<p>En el contexto de esta practica, el algoritmo realiza los siguientes pasos:</p>

<h4 id="1lectura-de-sensores"><b>1.Lectura de Sensores:<b></b></b></h4>

<p>Los sensores de infrarrojos detectan la intensidad de la línea en tres puntos: izquierda, centro y derecha. Según los valores recibidos, el sistema determina si el coche está alineado con la línea, desviado a la izquierda, o desviado a la derecha.</p>

<h4 id="2cálculo-del-error"><b>2.Cálculo del Error:<b></b></b></h4>

<p>Se calcula el error como la diferencia en las lecturas de los sensores izquierdo y derecho. Este error indica qué tan lejos está el coche de estar perfectamente alineado con la línea.</p>
<h4 id="3aplicación-del-control-pd"><b>3.Aplicación del Control PD:<b></b></b></h4>

<ul>
  <li>El error proporcional se multiplica por una constante de ganancia proporcional (Kp), lo que genera una corrección proporcional a la magnitud del error.</li>
  <li>El cambio del error (derivada del error) se multiplica por una constante de ganancia derivativa (Kd​), que suaviza las correcciones para evitar oscilaciones.</li>
</ul>

<h4 id="4ajuste-de-velocidades-de-los-motores"><b>4.Ajuste de Velocidades de los Motores:<b></b></b></h4>

<p>La corrección calculada se aplica para modificar las velocidades de los motores izquierdo y derecho, de manera que el coche pueda ajustar su trayectoria y mantenerse sobre la línea.</p>

<h4 id="5limitación-de-velocidades"><b>5.Limitación de Velocidades:<b></b></b></h4>

<p>Las velocidades calculadas se limitan para evitar valores fuera del rango permitido, asegurando un movimiento estable y seguro.</p>

<h2 id="video">Video</h2>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/follow_line/coche-clase.mp4" type="video/mp4" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<p> </p>

<p>Colaborador Jorge Barroso Saugar</p>]]></content><author><name></name></author><category term="embedded-and-real-time-systems" /><summary type="html"><![CDATA[RUST-EZE Descripción]]></summary></entry><entry><title type="html">P5 Monte Carlo Laser Localization</title><link href="http://localhost:4000/2024/12/02/p5-MCL.html" rel="alternate" type="text/html" title="P5 Monte Carlo Laser Localization" /><published>2024-12-02T00:00:00+01:00</published><updated>2024-12-02T00:00:00+01:00</updated><id>http://localhost:4000/2024/12/02/p5-MCL</id><content type="html" xml:base="http://localhost:4000/2024/12/02/p5-MCL.html"><![CDATA[<h2 id="description">Description</h2>

<p>The objective of this practice is to develop the algorithm of Monte Carlo to localizate the robot.</p>

<h2 id="monte-carlo-algorithm">Monte Carlo Algorithm</h2>

<h4 id="initialize">Initialize</h4>
<div style="display: flex; align-items: center;">
    <img src="/assets/videos/p5/inicializar.gif" alt="GIF divertido" width="400" />
    <div style="margin-left: 20px; text-align: right;">
        <p>
            The particles are initialized with a uniform random distribution with a noise:
            <br />
            <strong>INIT_XY_STD = 0.5</strong> <br />
            <strong>INIT_ANGLE_STD = 0.1</strong>
        </p>
    </div>
</div>

<p> </p>

<h4 id="step-1-propagation-motion-update">Step 1 Propagation (motion update)</h4>
<div style="display: flex; align-items: center;">
    <img src="/assets/videos/p5/propagacion.gif" alt="GIF divertido" width="400" />
    <div style="margin-left: 20px; text-align: right;">
        <p>
            The pose of particles move according to the movement of the robot with a little noise:
            <br />
            <strong>PROPAGATION_XY_NOISE_STD = 0.05</strong> <br />
            <strong>PROPAGATION_ANGLE_NOISE_STD = 0.01</strong>
        </p>
    </div>
</div>

<p> </p>

<h4 id="step-2-sensor-update-weight-estimation">Step 2 Sensor update (weight estimation)</h4>

<p>Following the Probabilistic Sensor Observation Model, the particles are assigned a weight depending on the difference between the laser sensor reading from the robot and a theoretical laser developed for each particle and each iteration.</p>

<p>The calculation of the probabilities is given by the following formula :</p>

<p><img src="/assets/images/p5/formula.png" alt="GIF divertido" width="400" /></p>

<p>Where the error represents the discrepancy between the actual sensor readings and the theoretical predictions for each particle, calculated based on their position and orientation in the map</p>

<p>The weights are normalized to ensure they form a probability distribution, making it easier to compare and interpret the likelihood of each particle accurately.</p>

<h4 id="step-3-resampling">Step 3 Resampling</h4>

<p>Select and give more importance to the most relevant particles, those which have more probability. Eliminating the irrelevant particles and duplicating the relevant ones</p>

<p> </p>

<h3 id="solution">Solution</h3>
<p>The most significant problem was the convergence of the particles, not because the algorithm wasn’t accurate, but because the computation time was very high, which did not allow for fast processing. Therefore, we can observe an initial solution without parallel processing and a final solution implementing parallel processing.</p>

<h4 id="result-without-multiprocessing">Result without multiprocessing</h4>

<div style="display: flex; align-items: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p5/sin_multi_video.mp4" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>
    <div style="margin-left: 20px; text-align: right;">
        <p>
            As we can see in the video, the computation time is quite high. This prevents the algorithm from fully converging, as it requires fast processing. Using fewer        particles and fewer virtual ray traces reduces the computation time, but it is still not enough.
            <br />
            <strong>N_PARTICLES = 500</strong> <br />
            <strong>LASER_NUM_BEAMS = 3</strong>
        </p>
    </div>
</div>

<p> </p>

<h4 id="result-with-multiprocessing">Result with multiprocessing</h4>

<div style="display: flex; align-items: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p5/multiprocesing_video.mp4" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>
    <div style="margin-left: 20px; text-align: right;">
        <p>
            By the other side, using multiprocessing to execute the virtuals ray traces parallel show that the computation time reduce significantly. So we can increase the number of virtual ray traces.
            <br />
            <strong>N_PARTICLES = 500</strong> <br />
            <strong>LASER_NUM_BEAMS = 5</strong>
        </p>
    </div>
</div>]]></content><author><name></name></author><category term="mobile-robotics" /><summary type="html"><![CDATA[Description]]></summary></entry><entry><title type="html">P3 Vending machine</title><link href="http://localhost:4000/2024/11/07/p3-maquina-expendedora.html" rel="alternate" type="text/html" title="P3 Vending machine" /><published>2024-11-07T00:00:00+01:00</published><updated>2024-11-07T00:00:00+01:00</updated><id>http://localhost:4000/2024/11/07/p3-maquina-expendedora</id><content type="html" xml:base="http://localhost:4000/2024/11/07/p3-maquina-expendedora.html"><![CDATA[<h2 id="description">Description</h2>
<p>La practica consiste en diseñar un controlador para una maquina expendedora que este basado en Arduino Uno. Para diseñar este controlador haremos uso de Arduino Threads, interrupciones, watchdog…</p>

<h2 id="hardware">Hardware</h2>

<p align="center">
  <img src="/assets/images/p3-empotrados/arduino.jpeg" alt="Arduino UNO" width="200" />
  <img src="/assets/images/p3-empotrados/lcd.webp" alt="LCD" width="200" />
  <img src="/assets/images/p3-empotrados/joystick.jpg" alt="Joystick" width="200" />
</p>
<p align="center">
  <b>Arduino UNO</b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <b>LCD</b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <b>Joystick</b>
</p>

<p align="center">
  <img src="/assets/images/p3-empotrados/dht11.jpeg" alt="Sensor DHT-11" width="200" />
  <img src="/assets/images/p3-empotrados/Hc-sr04.jpg" alt="Sensor HC-SR04" width="200" />
  <img src="/assets/images/p3-empotrados/boton.jpeg" alt="Botón" width="200" />
</p>
<p align="center">
  <b>Sensor DHT-11</b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <b>HC-SR04</b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <b>Botón</b>
</p>

<p align="center">
  <img src="/assets/images/p3-empotrados/leds.jpg" alt="LED Rojo" width="200" />
</p>
<p align="center">
  <b>LEDs</b> 
</p>

<p> </p>

<p>A partir de todos estos componentes obtenemos el siguiente esquema electrónico.</p>

<p align="center">
  <img src="/assets/images/p3-empotrados/Maquina_expendedora_bb.jpg" width="500" />
</p>

<h2 id="software">Software</h2>

<h3 id="clases">Clases</h3>
<h4 id="boot">Boot</h4>

<p>La clase boot es la clase encargada del arranque de la maquina expendedora. La clase Boot es una clase derivada de Thread que controla un pin led y una pantalla LCD, por lo tanto se ejecuta como “hilo independiente” que, una vez que acaba la secuencia de arranque, no se vuelve a ejecutar.</p>

<p>Al ejecutar se llama al metodo run la cual va alternando el estado de un pin y además muestra por la pantalla del lcd la cadena “CARGANDO…”.</p>

<p>Cuando el contador ha llegado a 3, se dejada de ejecutar el hilo y se remueve del controlador, ya que no volveremos a ejecutar dicha clase.</p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p3-empotrados/boot.mp4" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<p> </p>

<h4 id="sensors">Sensors</h4>

<p>La clase sensor es la clase encargada de la lectura del sensor de humedad y temperatura y humedad. La clase es una clase derivada de Thread, por lo que se ejecuta como “hilo” cada 5 segundos y nunca es removido debido a que siempre queremos tener monitorizado la temperatura y humedad de nuestra maquina para prevenir de posibles desastres.</p>

<p>En ella encontramos la inicialización del sensor, dos métodos que obtienen los valores de temperatura y humedad respectivamente y dos métodos que muestran los valores por el lcd.</p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p3-empotrados/sensors.mp4" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<p> </p>

<h4 id="menu">Menu</h4>

<p>La clase Menu es una clase que hereda de Thread y administra un menú de bebidas para una pantalla LCD controlada por un objeto LiquidCrystal. Contiene un arreglo de objetos Item (con nombre y precio), índices para la navegación del menú, y métodos para mostrar el menú en pantalla, cambiar precios y apagar la pantalla. El constructor inicializa el menú con 5 elementos predeterminados y configura el índice inicial en 0.</p>

<p>Los métodos permiten visualizar el elemento actual del menú, actualizar su precio y limpiar la pantalla LCD.</p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p3-empotrados/menu.mp4" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<p> </p>

<h4 id="admin">Admin</h4>

<p>La clase Admin hereda de Thread y gestiona un menú administrativo para un dispositivo con pantalla LCD controlada por un objeto LiquidCrystal. Contiene un arreglo de cadenas con 4 opciones de menú y un índice para navegar entre ellas.</p>

<p>Incluye un método para mostrar la opción actual del menú en la pantalla y otro para apagar (limpiar) la pantalla. Proporciona una interfaz simple para administrar funcionalidades como sensores y configuración.</p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p3-empotrados/admin.mp4" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<p> </p>

<h3 id="main">Main</h3>

<p>El main esta compuesto principalmente de las funciones :</p>

<ul>
  <li>
    <p><strong>setup</strong> en donde se inicializa el lcd, los pines para los leds y para las interrupciones, los intervalos de los threads en el controlador y el watchdog.</p>
  </li>
  <li>
    <p><strong>loop</strong> en donde se encuentra una maquina de estados finita.</p>
  </li>
</ul>

<p> </p>

<p align="center">
  <img src="/assets/images/p3-empotrados/state-machine.jpg" width="500" />
</p>

<p> </p>

<p>Como principales estados tenemos:</p>

<ul>
  <li>
    <p><strong>BOOT</strong>, estado que obtiene el contador de la clase BOOT y si ha llegado a 3 termina y transita al estado WAITING_CLIENT</p>
  </li>
  <li>
    <p><strong>WAITING_CLIENT</strong>, estado en el que el sensor de ultrasonidos recoge los valores de la distancia, en cuando hay un cliente a menos de un metro transita al estado IS_CLIENT</p>
  </li>
  <li>
    <p><strong>IS_CLIENT</strong>, estado que muestra durante 5 segundos la temperatura y humedad y transita al estado SHOW_MENU.</p>
  </li>
  <li>
    <p><strong>SHOW_MENU</strong>, estado que muestra el menú usando el método “showMenu” de la clase Menu, solo transita unicamente con la interrupción del botón del joystick.</p>
  </li>
  <li>
    <p><strong>PREPARE_PRODUCT</strong>, estado que simula la preparación del producto, en el se enciende progresivamente un led y muestra un mensaje por el lcd durnate un tiempo ramdon. Una vez finalizado muestra por el lcd que el producto esta listo y transita al estado WAITING_CLIENT.</p>
  </li>
</ul>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p3-empotrados/prepare-product.mp4" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<p> </p>

<p>En cualquier momento si queremos reiniciar y volver al estado WAITING_CLIENT se debe pulsar el botón en un rango de entre 2 y 3 segundos.</p>

<p>Con estos 5 estados tenemos el comportamiento a nivel de usuario de la maquina expendedora. Sin embargo, hay más comportamientos a nivel de administrados. Para transitar al estado de administrador se debe pulsar el botón durante 5 segundos.</p>

<ul>
  <li>
    <p><strong>ADMIN</strong>, estado que muestra el menú del administrados con las diferentes funcionalidades. Cada una se muestra como un sub-estado:</p>

    <ul>
      <li>
        <p><strong>SEE_TEMPERATURE</strong>, estado que muestra la temperatura y la humedad.</p>
      </li>
      <li>
        <p><strong>SEE_DISTANCE</strong>, estado que muestra los valores del sensor de ultrasonidos</p>
      </li>
      <li>
        <p><strong>COUNTER</strong>, estado que muestra el tiempo desde que se inicio.</p>
      </li>
      <li>
        <p><strong>CHANGE_PRICE</strong>, estado que cambia el precio de un producto, el cual tiene el sub-estado <strong>ASIGN_PRICE</strong> al cual solo transita unicamente con la interrupción del boton del joystick.</p>
      </li>
    </ul>
  </li>
</ul>

<p>Para transitar por los diferentes sub-estados al estado ADMIN unicamente se puede hacer mediante un movimiento del joystick en el eje X a la izquierda, una acción para “volver al menú”</p>

<p> </p>

<h4 id="watchdog">Watchdog</h4>

<p>Implementamos el watchdog como mecamismo de seguridad por si el sistema se queda bloqueado y no tiene el comportamiento deseado con un timer de 8 segundos.</p>

<p>Podemos observar su correcto funcionamiento ya que en el estado PREPARE_PRODUCT simulamos la preparación del producto con un delay random entre 4 y 8 segundos. Si no se resetea el watchdog, este va a reiniciar el sistema cuando el random de valor 8. En el siguiente video se puede observar lo comentado anteriormente.</p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p3-empotrados/watchdog.mp4" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>
<p> </p>

<h3 id="final-video">Final video</h3>

<p>Video final con el comportamiento completo de la Maquina expendedora.</p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p3-empotrados/complete-video.mp4" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>
</div>

<p> </p>

<h2 id="conclusion">Conclusion</h2>

<p>En conclusión, he obtado por esta implementación ya que el uso de <strong>clases</strong> nos facilita la posible implementación en otros proyectos, además de brindar mayor escalabilidad y modularidad a nuestro sistema, lo que permite un mantenimiento y actualización más eficientes. Por otro lado, el uso de <strong>threads</strong> nos permite ejecutar tareas en distintos periodos de tiempo sin necesidad de utilizar <strong>delays</strong>, mejorando significativamente la funcionalidad y la respuesta de nuestro <strong>sketch</strong>. Asi mismo, la implementación de <strong>interrupciones</strong> resulta fundamental para manejar eventos críticos en tiempo real, ya que estas permiten responder de manera inmediata a señales como los botones sin la necesidad de sondear constantemente, optimizando así el rendimiento del sistema y garantizando una mayor eficiencia.</p>]]></content><author><name></name></author><category term="embedded-and-real-time-systems" /><summary type="html"><![CDATA[Description La practica consiste en diseñar un controlador para una maquina expendedora que este basado en Arduino Uno. Para diseñar este controlador haremos uso de Arduino Threads, interrupciones, watchdog…]]></summary></entry><entry><title type="html">P4 Global Navigation</title><link href="http://localhost:4000/2024/10/28/p4-global-navigation.html" rel="alternate" type="text/html" title="P4 Global Navigation" /><published>2024-10-28T00:00:00+01:00</published><updated>2024-10-28T00:00:00+01:00</updated><id>http://localhost:4000/2024/10/28/p4-global-navigation</id><content type="html" xml:base="http://localhost:4000/2024/10/28/p4-global-navigation.html"><![CDATA[<h2 id="description">Description</h2>

<p>The objective of this practice is to develop a global navigation algorithm. The user marks a point on the map, and the car must reach it by the shortest path.</p>

<h2 id="hardware">Hardware</h2>

<p>We only have a car without any sensors. The only information we have is its current position.</p>

<h2 id="software">Software</h2>

<h3 id="wave-front-algorthim">Wave Front Algorthim</h3>
<p>For the car’s navigation, we developed a search algorithm based on the Wave Front Algorithm. This approach generates a cost map by expanding from the final target, assigned a cost of 0, to the car, which receives a cost of <em>n</em>.</p>

<p>To create the cost map, successors of each point are assigned a value equal to <em>cost + 1</em>, while diagonal successors are assigned a value of <em>cost + 1.41</em> (representing the Euclidean distance).</p>

<div style="text-align: center;">
    <img src="/assets/images/p4/Captura desde 2024-11-18 19-01-14.png" alt="car" style="width: 300px" />
</div>

<p>Additionally, obstacles and positions near them are assigned an extra cost to ensure that these points are avoided during navigation.</p>

<p>Once the search and the filling of the cost grid are completed, we display the map based on the cost grid. However, since the costs exceed 255 (the maximum white color), we need to normalize it so that it is displayed in a range from 0 to 255 based on the highest cost.</p>

<div style="text-align: center;">
    <img src="/assets/images/p4/Captura desde 2024-11-26 11-44-16.png" alt="car" style="width: 300px" />
</div>

<h3 id="gradient-algorthim">Gradient algorthim</h3>

<p>Once the cost map is generated, we implement a gradient-based navigation algorithm. It works using potential fields: obstacles act as “potential walls” that the robot avoids, while the target serves as a “potential well” that attracts it. By combining these walls and wells, a downward slope path is created, and the robot simply follows it to reach its destination.</p>

<p> </p>

<h3 id="videos">Videos</h3>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p4/grabacion-de-pantalla-desde-2024-11-18-10-37-27_hUPDu7c6.webm" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>

&nbsp;

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p4/grabacion-de-pantalla-desde-2024-11-18-10-44-35_0AQSLjZU.webm" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>

</div></div>]]></content><author><name></name></author><category term="mobile-robotics" /><summary type="html"><![CDATA[Description]]></summary></entry><entry><title type="html">P3 Obstacle Avoidance</title><link href="http://localhost:4000/2024/10/14/p3-obstacle_avoidance.html" rel="alternate" type="text/html" title="P3 Obstacle Avoidance" /><published>2024-10-14T00:00:00+02:00</published><updated>2024-10-14T00:00:00+02:00</updated><id>http://localhost:4000/2024/10/14/p3-obstacle_avoidance</id><content type="html" xml:base="http://localhost:4000/2024/10/14/p3-obstacle_avoidance.html"><![CDATA[<h2 id="description">Description</h2>

<p>The aim of this practice is to program a Formula 1 car which can avoid obstacle using Virtual Force Field system. In this case, we are going to develop a local navigating algorthims so that the car can move throw the circuit reaching the waypoints and avoiding the obstacles.</p>

<div style="text-align: center;">
    <img src="/assets/images/p3/coche.png" alt="car" style="width: 300px" />
</div>

<h2 id="hardware">Hardware</h2>

<p>The sensor is the laser LIDAR 180º and the actuators are as always the motor of the Formula 1 car.</p>

<h2 id="software">Software</h2>

<p>The strategy for this practice is develop a local navegation algorthim called VFF (Virtual Force Field). Is a navegation tecnice which allows our formula 1 car to move in a environment avoiding obstacles. Basically, the direction of the car is given by the attractive force generated by the position of the goal.</p>

<h3 id="vff">VFF</h3>
<p>The attractive vector is the vector drawn from the car’s position to the waypoint. This vector becomes very strong over long distances, so we will apply a force reducer.</p>

<p>The repulsive vector is the result of a weighted average of all the vectors drawn for all the angles of the laser. It is a weighted average because, in certain situations, such as very short distances, they do not have enough weight, and therefore the repulsive vector is not effective.</p>

<p>The resulting force should be the weighted average of both the attractive and repulsive vectors. Depending on the factors we assign to each, one vector will have more weight than the other. In this case, I found it more convenient to give greater weight to the repulsive vector so that it prioritizes avoiding obstacles.</p>

<h4 id="strategies-to-highlight">Strategies to highlight</h4>

<p>Reduce the size of the attractive vector, as at very high distances, it exerts too much force.</p>

<p>Increase the weight of small distances measured by the laser, so that when calculating the repulsive vector, these distances have more significance.</p>

<p>The linear velocity depends on the distance calculated as the magnitude of the resulting vector from the weighted average of both forces. This way, at short distances, the speed is reduced to allow for precise evasion.</p>

<p> </p>

<p>Here it is a video with the behavior of the formula 1 car.</p>

<div style="text-align: center;">
    <video width="500" controls="">
      <source src="/assets/videos/p3/cochevueltacompleta.webm" type="video/webm" />
      Tu navegador no soporta la reproducción de videos.
    </video>

</div>]]></content><author><name></name></author><category term="mobile-robotics" /><summary type="html"><![CDATA[Description]]></summary></entry></feed>